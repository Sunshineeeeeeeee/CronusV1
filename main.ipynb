{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/aleksandr/Desktop/Meta_Test.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing with 570771 rows\n",
      "After filtering trading hours: 282810 rows\n",
      "After cleaning outliers: 282301 rows\n",
      "Final clean dataset: 278585 rows\n",
      "\n",
      "Outlier counts by detection method:\n",
      "  zscore: 64\n",
      "  extreme_deviation: 69\n",
      "  isolated_point: 390\n",
      "  price_reversal: 93\n",
      "  timestamp_group: 34\n",
      "  price_velocity: 3703\n",
      "  suspicious_cluster: 52\n",
      "  wavelet_outlier: 24\n"
     ]
    }
   ],
   "source": [
    "from clean import preprocess_tick_data\n",
    "\n",
    "df_clean, df_diagnostics, outlier_counter = preprocess_tick_data(df)\n",
    "df = df_clean\n",
    "df = df.drop(columns=\"VOLATILITY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volatility estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating advanced tick-level volatility for 278585 ticks...\n",
      "Processing 278585 ticks...\n",
      "Completed volatility estimation for 278585 ticks\n",
      "Completed advanced tick-level volatility estimation\n"
     ]
    }
   ],
   "source": [
    "from volatility_v1 import estimate_advanced_volatility\n",
    "\n",
    "df = estimate_advanced_volatility(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Value</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.24</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.214653e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.056060e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.217889e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.11</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.402309e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.10</td>\n",
       "      <td>249.0</td>\n",
       "      <td>3.357669e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Timestamp   Value  Volume    Volatility\n",
       "0 2025-01-30 09:30:00.740000+00:00  694.24    13.0  9.214653e-17\n",
       "1 2025-01-30 09:30:00.740000+00:00  694.17    15.0  2.056060e-03\n",
       "2 2025-01-30 09:30:00.740000+00:00  694.17    15.0  3.217889e-03\n",
       "3 2025-01-30 09:30:00.740000+00:00  694.11     8.0  2.402309e-03\n",
       "4 2025-01-30 09:30:00.740000+00:00  694.10   249.0  3.357669e-03"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['return', \"SYMBOL\", \"emd_vol\", \"sv_vol\", 'log_price', 'smooth_vol', 'raw_vol'], inplace= True)\n",
    "df.rename(columns={'filtered_vol' : 'Volatility', \n",
    "                  'TIMESTAMP':'Timestamp',\n",
    "                   'VALUE' : 'Value',\n",
    "                   'VOLUME' : 'Volume'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Only Transformer Feature engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be trained and saved to: /Users/aleksandr/code/scripts/CronusV1/Feature_engineering/saved_models\n",
      "Using causal mode. (Suitable for real-time applications)\n",
      "Setting up feature extractor...\n",
      "Extracting microstructure features...\n",
      "Skip outlier clipping for Volatility: 596\n",
      "Skip outlier clipping for price_change: 596\n",
      "Skip outlier clipping for log_return: 596\n",
      "Skip outlier clipping for time_delta: 596\n",
      "Skip outlier clipping for trade_direction: 596\n",
      "Skip outlier clipping for is_buy: 596\n",
      "Skip outlier clipping for tick_imbalance: 596\n",
      "Skip outlier clipping for jump_diffusion: 596\n",
      "Skip outlier clipping for jump_magnitude: 596\n",
      "Skip outlier clipping for jump_arrival: 596\n",
      "Skip outlier clipping for kyle_lambda: 596\n",
      "Skip outlier clipping for orderflow_imbalance: 596\n",
      "Skip outlier clipping for momentum_short: 596\n",
      "Skip outlier clipping for momentum_medium: 596\n",
      "Skip outlier clipping for momentum_long: 596\n",
      "Skip outlier clipping for price_range_short: 596\n",
      "Skip outlier clipping for price_range_medium: 596\n",
      "Skip outlier clipping for price_range_long: 596\n",
      "Skip outlier clipping for volatility_per_volume: 596\n",
      "Skip outlier clipping for bipower_var_short: 596\n",
      "Skip outlier clipping for bipower_var_medium: 596\n",
      "Skip outlier clipping for jump_ratio: 596\n",
      "Skip outlier clipping for bipower_var_long: 596\n",
      "Skip outlier clipping for staggered_bipower_var: 596\n",
      "Skip outlier clipping for min_bipower_var_10: 596\n",
      "Skip outlier clipping for min_bipower_var_30: 596\n",
      "Skip outlier clipping for min_bipower_var_50: 596\n",
      "Extracted 20 microstructure features\n",
      "Preparing data tensors...\n",
      "Skip outlier clipping for Volatility: 596\n",
      "Skip outlier clipping for price_change: 596\n",
      "Skip outlier clipping for log_return: 596\n",
      "Skip outlier clipping for time_delta: 596\n",
      "Skip outlier clipping for trade_direction: 596\n",
      "Skip outlier clipping for is_buy: 596\n",
      "Skip outlier clipping for tick_imbalance: 596\n",
      "Skip outlier clipping for jump_diffusion: 596\n",
      "Skip outlier clipping for jump_magnitude: 596\n",
      "Skip outlier clipping for jump_arrival: 596\n",
      "Skip outlier clipping for kyle_lambda: 596\n",
      "Skip outlier clipping for orderflow_imbalance: 596\n",
      "Skip outlier clipping for momentum_short: 596\n",
      "Skip outlier clipping for momentum_medium: 596\n",
      "Skip outlier clipping for momentum_long: 596\n",
      "Skip outlier clipping for price_range_short: 596\n",
      "Skip outlier clipping for price_range_medium: 596\n",
      "Skip outlier clipping for price_range_long: 596\n",
      "Skip outlier clipping for volatility_per_volume: 596\n",
      "Skip outlier clipping for bipower_var_short: 596\n",
      "Skip outlier clipping for bipower_var_medium: 596\n",
      "Skip outlier clipping for jump_ratio: 596\n",
      "Skip outlier clipping for bipower_var_long: 596\n",
      "Skip outlier clipping for staggered_bipower_var: 596\n",
      "Skip outlier clipping for min_bipower_var_10: 596\n",
      "Skip outlier clipping for min_bipower_var_30: 596\n",
      "Skip outlier clipping for min_bipower_var_50: 596\n",
      "Data shapes - Values: torch.Size([9951, 50, 20]), Time: torch.Size([9951, 50, 8])\n",
      "Training new model...\n",
      "Model created with 546000 parameters\n",
      "Epoch 5/10 - Loss: 0.5600 - Contrastive: 1.5538 - Entropy: -0.0801 - Diversity: -3.7868\n",
      "Epoch 10/10 - Loss: 0.2186 - Contrastive: 1.5408 - Entropy: -0.0944 - Diversity: -3.7663\n",
      "Model saved to /Users/aleksandr/code/scripts/CronusV1/Feature_engineering/saved_models/regime_model_20250410_1958_causal.pt\n",
      "Extracting features...\n",
      "Extracted features shape: (9951, 16)\n",
      "Features saved to /Users/aleksandr/code/scripts/CronusV1/Feature_engineering/saved_models/regime_features.csv\n",
      "\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from Feature_engineering.feature_model_v1 import process_market_data\n",
    "\n",
    "# Set up paths and configuration\n",
    "df = df[:10000]\n",
    "model_dir = '/Users/aleksandr/code/scripts/CronusV1/Feature_engineering/saved_models'\n",
    "\n",
    "# Process data with the model \n",
    "features_df, model = process_market_data(\n",
    "    df=df,\n",
    "    model_dir=model_dir,\n",
    "    retrain=True,\n",
    "    num_epochs=10,\n",
    "    context_length=50,\n",
    "    num_attention_heads=8,\n",
    "    num_encoder_layers=4,\n",
    "    causal=True,\n",
    "    temperature=0.5,\n",
    "    grad_clip_norm=1.0\n",
    ")\n",
    "\n",
    "print(f\"\\nProcess completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ttest = pd.read_csv(\"/Users/aleksandr/code/scripts/CronusV1/Feature_engineering/saved_models/regime_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9951, 20)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 9951 rows and 16 features\n",
      "Topology-First Feature Selection: min_gain=0.06, corr_threshold=0.65, max_features=7\n",
      "Step 1: Computing feature metrics...\n",
      "  - Computing topological diversity scores...\n",
      "  - Computing information uniqueness scores...\n",
      "  - Computing signal stability scores...\n",
      "  - Computing orthogonal variance scores...\n",
      "Step 2: Combining scores with weights: {'diversity': 0.35, 'uniqueness': 0.3, 'signal_to_noise': 0.2, 'orthogonal_variance': 0.15}\n",
      "\n",
      "Top features by topology-focused score:\n",
      "  1. regime_feature_11: 0.8475\n",
      "  2. regime_feature_10: 0.8404\n",
      "  3. regime_feature_7: 0.8346\n",
      "  4. regime_feature_13: 0.7945\n",
      "  5. regime_feature_1: 0.7848\n",
      "  6. regime_feature_4: 0.7762\n",
      "  7. regime_feature_6: 0.7648\n",
      "  8. regime_feature_16: 0.7645\n",
      "  9. regime_feature_3: 0.7606\n",
      "  10. regime_feature_14: 0.7413\n",
      "\n",
      "Step 4: Calculating information gain (before correlation filtering)...\n",
      "Reached maximum number of features (7)\n",
      "After information gain: 7 features with cumulative gain: 2.5125\n",
      "\n",
      "Step 5: Applying correlation filtering (threshold=0.65)...\n",
      "  High correlation (-0.77) between regime_feature_10 and regime_feature_1\n",
      "  → Removed regime_feature_1 (score: 0.7848)\n",
      "  High correlation (-0.77) between regime_feature_7 and regime_feature_1\n",
      "  High correlation (-0.68) between regime_feature_7 and regime_feature_4\n",
      "  → Removed regime_feature_4 (score: 0.7762)\n",
      "After correlation filtering: 5/7 features remain\n",
      "\n",
      "Final selection: 5 features\n",
      "\n",
      "Selected features - Key metrics:\n",
      "  - Total variance captured: 93.87%\n",
      "  - Average correlation: 0.4339\n",
      "  - Maximum correlation: 0.6063\n",
      "  - PC1 explains: 50.22% of variance\n",
      "  - PC2 explains: 32.51% of variance\n",
      "  - PC3 explains: 11.14% of variance\n",
      "\n",
      "Selected features (ranked by importance):\n",
      "  1. regime_feature_11 (score: 0.8475)\n",
      "  2. regime_feature_10 (score: 0.8404)\n",
      "  3. regime_feature_7 (score: 0.8346)\n",
      "  4. regime_feature_13 (score: 0.7945)\n",
      "  5. regime_feature_6 (score: 0.7648)\n",
      "\n",
      "Feature selection for TDA completed!\n",
      "\n",
      "Selected 5 features:\n"
     ]
    }
   ],
   "source": [
    "from Feature_engineering.feature_selection import run_feature_selection\n",
    "\n",
    "filtered_df = run_feature_selection(\n",
    "    df=ttest,  \n",
    "    min_gain_threshold=0.06,  \n",
    "    max_features=7,\n",
    "    correlation_threshold=0.65,\n",
    "    verbose=True,\n",
    "    weights={\n",
    "        'diversity': 0.35,       \n",
    "        'uniqueness': 0.30,      \n",
    "        'signal_to_noise': 0.20, \n",
    "        'orthogonal_variance': 0.15  \n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nSelected {filtered_df.shape[1] - 4} features:\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9951, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Regime Identification in TDA Pipeline\n",
    "\n",
    "## Core Concept\n",
    "Enhance the standard TDA-based regime identification by introducing hierarchical labeling to capture both primary market regimes and their sub-regimes using a tuple representation.\n",
    "\n",
    "## Enhanced Mapper Function Implementation\n",
    "1. **Integrated Hierarchical Mapper**\n",
    "   - Extend the KMapper class to include hierarchical regime identification\n",
    "   - Implement a single enhanced mapper function that handles both parent and child regimes\n",
    "   - Maintain a single responsibility focused on structure identification at multiple scales\n",
    "   - Return complete hierarchical labeling in one pass\n",
    "\n",
    "2. **Primary-to-Secondary Regime Process**\n",
    "   - First identify primary regimes using standard topological analysis\n",
    "   - Within the same mapper function, apply HDBSCAN to each primary regime\n",
    "   - Ensure algorithmic consistency through integrated parameter handling\n",
    "   - Optimize computational efficiency by avoiding redundant data passing\n",
    "\n",
    "3. **Hierarchical Labeling System**\n",
    "   - Replace scalar labels with tuple representation: (parent_regime, child_regime)\n",
    "   - Main regimes without sub-clustering: (1,0), (2,0), (3,0), etc.\n",
    "   - Sub-regimes of regime 1: (1,1), (1,2), (1,3), etc.\n",
    "   - Regimes failing quality checks remain as (n,0) without sub-divisions\n",
    "\n",
    "4. **Direct Tensor Output**\n",
    "   - Have mapper function directly output tensor with hierarchical labels\n",
    "   - Eliminate need for separate post-processing modules\n",
    "   - Simplify code architecture and data flow\n",
    "   - Reduce pipeline complexity and improve maintainability\n",
    "\n",
    "## Window Treatment for Pipeline\n",
    "- **Critical**: Maintain entire parent regimes as single windows\n",
    "- Do NOT split parent regimes into separate sub-regime windows\n",
    "- Use sub-regime labels as conditioning metadata only\n",
    "- Preserve temporal continuity and transition patterns between sub-regimes\n",
    "- Keep statistical power by using larger parent-level windows\n",
    "\n",
    "## Enhanced Sub-Regime Quality Controls\n",
    "- **Minimum Size Enforcement**: Require at least 5% of parent regime points to form valid sub-regime\n",
    "- **Statistical Validation**: Apply silhouette score threshold (>0.3) to ensure meaningful clusters\n",
    "- **Automatic Optimization**: Use gap statistic to determine optimal number of sub-regimes\n",
    "- **Density-Based Approach**: HDBSCAN naturally handles varying cluster densities and outliers\n",
    "- **Bayesian GMM Alternative**: For regimes with Gaussian-like distribution characteristics\n",
    "\n",
    "## Code Architecture Benefits\n",
    "- **Data Efficiency**: Minimizes redundant data passing between components\n",
    "- **Consistency**: Ensures parameter and methodology alignment across levels\n",
    "- **Simplicity**: Reduces distinct components to maintain and debug\n",
    "- **Cohesion**: Keeps related functionality together for better maintainability\n",
    "- **Performance**: Reduces overhead from multiple processing stages\n",
    "\n",
    "## Technical Benefits\n",
    "- Preserves richer topological structure identified by TDA\n",
    "- Captures nested behavior patterns within major regimes\n",
    "- Enables more precise conditioning in the diffusion model\n",
    "- Provides structured hierarchical information for Titan's memory mechanisms\n",
    "- Maintains window sizes sufficient for robust statistical analysis\n",
    "\n",
    "## Implementation Notes\n",
    "- Adaptive sub-regime identification based on parent regime characteristics\n",
    "- Consider NO sub-regimes when parent regime is already coherent (maintain as (n,0))\n",
    "- Implement visualization tools to display hierarchical structure\n",
    "- Calculate transition probabilities between sub-regimes for additional insights\n",
    "\n",
    "## Integration with Subsequent Pipeline Steps\n",
    "- **For Diffusion Model**: \n",
    "  - Feed entire parent regime windows to the model\n",
    "  - Use tuple labels as conditioning information only\n",
    "  - Maintain temporal coherence within parent windows\n",
    "  - Learn to denoise and represent parent windows with awareness of internal structure\n",
    "\n",
    "- **For Tensor Structure**: \n",
    "  - Adjust conditioning dimensions to accommodate tuple representation\n",
    "  - Use parent regime for window boundaries\n",
    "  - Incorporate sub-regime information as metadata within windows\n",
    "\n",
    "- **For Titan**: \n",
    "  - Leverage hierarchical structure for more precise memory-based pattern matching\n",
    "  - Utilize sub-regime transitions as potential predictive signals\n",
    "\n",
    "## Key Metrics to Validate Approach\n",
    "- Sub-regime stability over time\n",
    "- Predictive power improvement compared to flat regime structure\n",
    "- Transition patterns between sub-regimes\n",
    "- Information gain from hierarchical representation\n",
    "- Statistical significance of identified sub-regimes\n",
    "\n",
    "## Priority Development Tasks\n",
    "1. Extend KMapper class with hierarchical mapping capability\n",
    "2. Implement integrated HDBSCAN sub-regime detection\n",
    "3. Add quality control mechanisms within mapper function\n",
    "4. Create direct tensor output with hierarchical labels\n",
    "5. Update visualizations to show hierarchical structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window based contidioned on volatilty regime diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Market Regime Representation Using Diffusion Models\n",
    "\n",
    "## Core Concept\n",
    "Extend traditional feature representation with diffusion models to create a 4D tensor representation of market data, capturing both explicit features and latent regime characteristics for high-frequency trading applications.\n",
    "\n",
    "## Data Structure\n",
    "- **Input**: Tensor of shape (i,j,k)\n",
    "  - i = window number (segmented by volatility regimes)\n",
    "  - j = datapoints within regime window \n",
    "  - k = feature dimensions\n",
    "- **Output**: Enhanced tensor with either:\n",
    "  - Extended features: (i,j,q) where q > k\n",
    "  - Multi-scale representation: (i,(j,k,r)) with new dimension r for latent features\n",
    "\n",
    "## Key Benefits\n",
    "1. Captures subtle market microstructure patterns specific to volatility regimes\n",
    "2. Enhances regime transition identification\n",
    "3. Creates richer representations for Titan's memory mechanisms \n",
    "4. Improves pattern recognition across similar historical regimes\n",
    "5. Preserves both raw features and their latent abstractions\n",
    "\n",
    "## Technical Implementation Path\n",
    "\n",
    "### Phase 1: Diffusion Model Design\n",
    "- Design diffusion model architecture for window-level processing\n",
    "- Implement forward diffusion process (noise injection)\n",
    "- Implement reverse diffusion process (denoising)\n",
    "- Incorporate contrastive learning objective to enhance regime separation\n",
    "\n",
    "### Phase 2: Latent Space Extension\n",
    "- Extract intermediate latent representations from diffusion model\n",
    "- Define structure of additional dimension to capture regime-specific patterns\n",
    "- Implement dimension extension procedure\n",
    "- Design feature fusion mechanism to combine original and latent features\n",
    "\n",
    "### Phase 3: Training Pipeline\n",
    "- Train diffusion model on windows from similar volatility regimes\n",
    "- Fine-tune with contrastive loss to enhance regime separation\n",
    "- Evaluate quality of latent representations using clustering metrics\n",
    "- Optimize hyperparameters for regime distinction\n",
    "\n",
    "### Phase 4: Integration with Titan Model\n",
    "- Structure 4D tensor as input for Titan's memory mechanisms\n",
    "- Configure Titan to utilize both raw features and latent representations\n",
    "- Optimize memory update parameters for enhanced pattern recognition\n",
    "- Evaluate improvements in prediction accuracy and regime identification\n",
    "\n",
    "## Evaluation Metrics\n",
    "1. Latent space clustering quality (silhouette score, within-cluster variance)\n",
    "2. Regime transition detection accuracy\n",
    "3. Prediction performance across different volatility regimes\n",
    "4. Memory utilization efficiency in Titan architecture\n",
    "5. Trading strategy performance metrics\n",
    "\n",
    "## Considerations\n",
    "- Carefully balance complexity vs. inference speed for HFT applications\n",
    "- Focus on regime transition periods for maximum trading advantage\n",
    "- Structure inference pipeline to minimize latency in production\n",
    "- Design ablation studies to quantify value of latent dimension extension\n",
    "\n",
    "## Implementation Priority\n",
    "1. Design and implement basic diffusion model for window denoising\n",
    "2. Add contrastive learning component for regime separation\n",
    "3. Implement latent feature extraction mechanism\n",
    "4. Integrate with Titan architecture\n",
    "5. Optimize for production performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
